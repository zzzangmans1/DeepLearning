# 17장 딥러닝을 이용한 자연어 처리

애플의 시리, 구글의 어시스턴스, 아마존의 알렉사나 네이버의 클로바까지 AI 비서라고 불리는 대화형 인공지능이 서로 경쟁하고 있습니다.
업무를 도와주고 삶의 질을 높여주는 인공지능 비서 서비스를 누구나 사용하는 시대가 왔습니다.
스마트폰이나 스피커, 앱 같은 형태로 보급되는 인공지능 비서가 갖추어야 할 필수 능력은 사람의 언어를 이해하는 것입니다.
문장을 듣고 무엇을 의미하는지 알아야 서비스를 제공해 줄 수 있기 때문이지요.
이 장에서는 이러한 능력을 만들어 주는 자연어 처리(Natural Language Processing, NLP)의 기본을 배울 것입니다.
자연어란 우리가 평소에 말하는 음성이나 텍스트를 의미합니다. 즉, 자연어 처리는 이러한 음성이나 텍스트를 컴퓨터가 인식하고 처리하는 것입니다.

## 1 텍스트의 토큰화

먼저 해야 할 일은 텍스트를 잘게 나누는 것입니다.
입력할 텍스트가 준비되면 이를 단어별, 문장별, 형태소별로 나눌 수 있는데, 이렇게 작게 나누어진 하나의 단위를 **토큰**이라고 합니다.
그래서 입력된 텍스트를 잘게 나누는 과정을 **토큰화**라고 합니다.
예를들어 다음 문장이 주어졌다고 가정해 봅시다.

'해보지 않으면 해낼 수 없다'

케라스가 제공하는 text 모듈의 **text_to_world_sequence()** 함수를 사용하면 문장을 단어 단위로 쉽게 나눌 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178199918-81ec167c-614a-45c0-8395-8267f7f3b08b.png)

![image](https://user-images.githubusercontent.com/52357235/178199904-e820b732-6fd4-4a79-b594-522b7ec89e50.png)

이렇게 주어진 텍스트를 단어 단위로 쪼개고 나면 이를 이용해 여러 가지를 할 수 있습니다.
예를 들어 각 단어가 몇 번이나 중복해서 쓰였는지 알 수 있습니다.
단어의 빈도수를 알면 텍스트에서 중요한 역할을 하는 단어를 파악할 수 있습니다.
따라서 텍스트를 단어 단위로 쪼개는 것은 가장 많이 쓰이는 전처리 과정입니다.

케라스의 **Tokenizer()** 함수를 사용하면 단어의 빈도수를 쉽게 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178201504-ae1a7405-65d8-44f2-9c86-118a132e039e.png)

![image](https://user-images.githubusercontent.com/52357235/178201542-b39cbfa1-0d6d-4831-9adb-01ce48de5944.png)

**document_count()** 함수를 이용하면 총 몇 개의 문장이 들어 있는지도 셀 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178203336-e03f85cb-b43e-46a0-b8e8-a035d02d8b80.png)

**word_docs()** 함수를 통해 각 단어들이 몇 개의 문장에 나오는지 세어서 출력할 수도 있습니다.
출력되는 순서는 랜덤입니다.

![image](https://user-images.githubusercontent.com/52357235/178203920-fcc5dbea-8119-4313-be1d-705134924f60.png)

![image](https://user-images.githubusercontent.com/52357235/178204061-991a163c-ea94-479b-9b41-cbd3875c0398.png)

각 단어에 매겨진 인덱스 값을 출력하려면 **word_index()** 함수를 사용하면 됩니다.

![image](https://user-images.githubusercontent.com/52357235/178205265-f2778446-98a6-450b-b7a9-c576289440de.png)

![image](https://user-images.githubusercontent.com/52357235/178205291-7df57f26-8bbe-4b3b-b8a9-eccd8e426589.png)

## 2 단어의 원-핫 인코딩

단순히 단어의 출현 빈도만 가지고는 해당 단어가 문장의 어디에서 왔는지, 각 단어의 순서는 어떠했는지 등에 관한 정보를 얻을 수 없습니다.
단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지 알아보는 방법이 필요합니다.
이러한 기법 중에서 가장 기본적인 방법은 **원-핫 인코딩**을 알아보겠습니다.

![image](https://user-images.githubusercontent.com/52357235/178206917-6c5a7721-8c83-4458-867a-254b4b0a0b7e.png)

이제 각 단어를 원-핫 인코딩 방식으로 표현해 보겠습니다.
케라스에서 제공하는 Tokenizer의 **texts_to_sequences()** 함수를 사용해서 앞서 만들어진 토큰의 인덱스로만 채워진 새로운 배열을 만들어 줍니다.

![image](https://user-images.githubusercontent.com/52357235/178210532-432ae6ca-3695-48e5-82eb-7db0004218dd.png)

이제 1~6의 정수로 인덱스되어 있는 것을 0과 1로만 이루어진 배열로 바꾸어 주는 **to_categorical()** 함수를 사용해 원-핫 인코딩 과정을 진행합니다.
배열 맨 앞에 0이 추가되므로 단어 수보다 1이 더 많에 인덱스 숫자를 잡아 주는 것에 유의하기 바랍니다.

![image](https://user-images.githubusercontent.com/52357235/178211624-a93a45fd-6906-4518-9bda-bf92b68e195e.png)

## 3 단어 임베딩

원-핫 인코딩과 함께 공부해야 할 것이 한 가지 더 있습니다. 
원-핫 인코딩을 그대로 사용하면 벡터의 길이가 너무 길어진다는 단점이 있습니다.
예를 들어 1만 개의 단어 토큰으로 이루어진 말뭉치를 다룬다고 할 때, 이 데이터를 원-핫 인코딩으로 벡터화하면 9,999개의 0과 하나의 1로 이루어진 단어 벡터를 1만 개나 만들어야 합니다.
이러한 공간적 낭비를 해결하기 위해 등장한 것이 **단어 임베딩** 이라는 방법입니다.
**단어 임베딩**은 주어진 배열을 정해진 길이로 압축시킵니다.
얻은 결과가 밀집된 정보를 가지고 있고 공간의 낭비가 적다는 것을 알 수 있습니다. 
이러한 결과가 가능한 이유는 각 단어 간의 유사도를 계산했기 때문입니다.
예를 들어 happy라는 단어는 bad보다 good에 더 가깝고, cat이라는 단어는 good보다는 dog에 가깝가는 것을 고려해 각 배열을 새로운 수치로 바꾸어 주는 것입니다.
이 과정은 케라스에서 제공하는 **Embedding()** 함수를 사용하면 간단히 해낼 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178214899-f039b4e0-1a71-4a6d-9306-d03561005887.png)


