# 17장 딥러닝을 이용한 자연어 처리

애플의 시리, 구글의 어시스턴스, 아마존의 알렉사나 네이버의 클로바까지 AI 비서라고 불리는 대화형 인공지능이 서로 경쟁하고 있습니다.
업무를 도와주고 삶의 질을 높여주는 인공지능 비서 서비스를 누구나 사용하는 시대가 왔습니다.
스마트폰이나 스피커, 앱 같은 형태로 보급되는 인공지능 비서가 갖추어야 할 필수 능력은 사람의 언어를 이해하는 것입니다.
문장을 듣고 무엇을 의미하는지 알아야 서비스를 제공해 줄 수 있기 때문이지요.
이 장에서는 이러한 능력을 만들어 주는 자연어 처리(Natural Language Processing, NLP)의 기본을 배울 것입니다.
자연어란 우리가 평소에 말하는 음성이나 텍스트를 의미합니다. 즉, 자연어 처리는 이러한 음성이나 텍스트를 컴퓨터가 인식하고 처리하는 것입니다.

## 1 텍스트의 토큰화

먼저 해야 할 일은 텍스트를 잘게 나누는 것입니다.
입력할 텍스트가 준비되면 이를 단어별, 문장별, 형태소별로 나눌 수 있는데, 이렇게 작게 나누어진 하나의 단위를 **토큰**이라고 합니다.
그래서 입력된 텍스트를 잘게 나누는 과정을 **토큰화**라고 합니다.
예를들어 다음 문장이 주어졌다고 가정해 봅시다.

'해보지 않으면 해낼 수 없다'

케라스가 제공하는 text 모듈의 **text_to_world_sequence()** 함수를 사용하면 문장을 단어 단위로 쉽게 나눌 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178199918-81ec167c-614a-45c0-8395-8267f7f3b08b.png)

![image](https://user-images.githubusercontent.com/52357235/178199904-e820b732-6fd4-4a79-b594-522b7ec89e50.png)

이렇게 주어진 텍스트를 단어 단위로 쪼개고 나면 이를 이용해 여러 가지를 할 수 있습니다.
예를 들어 각 단어가 몇 번이나 중복해서 쓰였는지 알 수 있습니다.
단어의 빈도수를 알면 텍스트에서 중요한 역할을 하는 단어를 파악할 수 있습니다.
따라서 텍스트를 단어 단위로 쪼개는 것은 가장 많이 쓰이는 전처리 과정입니다.

케라스의 **Tokenizer()** 함수를 사용하면 단어의 빈도수를 쉽게 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178201504-ae1a7405-65d8-44f2-9c86-118a132e039e.png)

![image](https://user-images.githubusercontent.com/52357235/178201542-b39cbfa1-0d6d-4831-9adb-01ce48de5944.png)

**document_count()** 함수를 이용하면 총 몇 개의 문장이 들어 있는지도 셀 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178203336-e03f85cb-b43e-46a0-b8e8-a035d02d8b80.png)

**word_docs()** 함수를 통해 각 단어들이 몇 개의 문장에 나오는지 세어서 출력할 수도 있습니다.
출력되는 순서는 랜덤입니다.

![image](https://user-images.githubusercontent.com/52357235/178203920-fcc5dbea-8119-4313-be1d-705134924f60.png)

![image](https://user-images.githubusercontent.com/52357235/178204061-991a163c-ea94-479b-9b41-cbd3875c0398.png)

각 단어에 매겨진 인덱스 값을 출력하려면 **word_index()** 함수를 사용하면 됩니다.

![image](https://user-images.githubusercontent.com/52357235/178205265-f2778446-98a6-450b-b7a9-c576289440de.png)

![image](https://user-images.githubusercontent.com/52357235/178205291-7df57f26-8bbe-4b3b-b8a9-eccd8e426589.png)

## 2 단어의 원-핫 인코딩

단순히 단어의 출현 빈도만 가지고는 해당 단어가 문장의 어디에서 왔는지, 각 단어의 순서는 어떠했는지 등에 관한 정보를 얻을 수 없습니다.
단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지 알아보는 방법이 필요합니다.
이러한 기법 중에서 가장 기본적인 방법은 **원-핫 인코딩**을 알아보겠습니다.

![image](https://user-images.githubusercontent.com/52357235/178206917-6c5a7721-8c83-4458-867a-254b4b0a0b7e.png)

이제 각 단어를 원-핫 인코딩 방식으로 표현해 보겠습니다.
케라스에서 제공하는 Tokenizer의 **texts_to_sequences()** 함수를 사용해서 앞서 만들어진 토큰의 인덱스로만 채워진 새로운 배열을 만들어 줍니다.

![image](https://user-images.githubusercontent.com/52357235/178210532-432ae6ca-3695-48e5-82eb-7db0004218dd.png)

이제 1~6의 정수로 인덱스되어 있는 것을 0과 1로만 이루어진 배열로 바꾸어 주는 **to_categorical()** 함수를 사용해 원-핫 인코딩 과정을 진행합니다.
배열 맨 앞에 0이 추가되므로 단어 수보다 1이 더 많에 인덱스 숫자를 잡아 주는 것에 유의하기 바랍니다.

![image](https://user-images.githubusercontent.com/52357235/178211624-a93a45fd-6906-4518-9bda-bf92b68e195e.png)

## 3 단어 임베딩

원-핫 인코딩과 함께 공부해야 할 것이 한 가지 더 있습니다. 
원-핫 인코딩을 그대로 사용하면 벡터의 길이가 너무 길어진다는 단점이 있습니다.
예를 들어 1만 개의 단어 토큰으로 이루어진 말뭉치를 다룬다고 할 때, 이 데이터를 원-핫 인코딩으로 벡터화하면 9,999개의 0과 하나의 1로 이루어진 단어 벡터를 1만 개나 만들어야 합니다.
이러한 공간적 낭비를 해결하기 위해 등장한 것이 **단어 임베딩** 이라는 방법입니다.
**단어 임베딩**은 주어진 배열을 정해진 길이로 압축시킵니다.
얻은 결과가 밀집된 정보를 가지고 있고 공간의 낭비가 적다는 것을 알 수 있습니다. 
이러한 결과가 가능한 이유는 각 단어 간의 유사도를 계산했기 때문입니다.
예를 들어 happy라는 단어는 bad보다 good에 더 가깝고, cat이라는 단어는 good보다는 dog에 가깝가는 것을 고려해 각 배열을 새로운 수치로 바꾸어 주는 것입니다.
이 과정은 케라스에서 제공하는 **Embedding()** 함수를 사용하면 간단히 해낼 수 있습니다.

![image](https://user-images.githubusercontent.com/52357235/178214899-f039b4e0-1a71-4a6d-9306-d03561005887.png)

Embedding() 함수는 입력과 출력의 크기를 나타내는 두 개의 매개변수가 있어야 합니다.
앞 예제에서 Embedding(16, 4)는 입력될 총 단어 수는 16, 임베딩 후 출력되는 벡터 크기는 4로 하겠다는 의미입니다.
여기에 단어를 매번 얼마나 입력할지 추가로 지정할 수 있습니다.
Embedding(16, 4, input_length=2)라고 하면 총 입력되는 단어 수는 16개이지만 매번 두 개씩만 넣겠다는 의미입니다.

## 4 텍스트를 읽고 긍정, 부정 예측하기

실습해 볼 과제는 영화를 보고 남긴 리뷰를 딥러닝 모델로 학습해서 각 리뷰가 긍정적인지 부정적인지를 예측하는 것입니다.
먼저 짧은 리뷰 열 개를 불러와 각각 긍정이면 1이라는 클래스를, 부정적이면 0이라는 클래스로 지정합니다.

![image](https://user-images.githubusercontent.com/52357235/178218593-896d903f-3c01-44bb-9532-760401b86224.png)

그다음 앞서 배운 토큰화 과정을 진행합니다.
케라스에서 제공하는 Tokenizer() 함수의 **fit_on_texts**를 이용해 각 단어를 하나의 토큰으로 변환합니다.

![image](https://user-images.githubusercontent.com/52357235/178219238-e6f27651-a97d-466d-8b39-95a51c5e49dc.png)

![image](https://user-images.githubusercontent.com/52357235/178219272-77a2c3c2-d34a-47ed-8e19-290d74a4c1ab.png)

이제 토큰에 지정된 인덱스로 새로운 배열을 생성합니다.

![image](https://user-images.githubusercontent.com/52357235/178220121-47c2bd23-3504-4dbb-b9e0-ec511f5aafa5.png)

![image](https://user-images.githubusercontent.com/52357235/178220143-74ed0df2-9859-472c-96a2-659563596fbd.png)

각 단어가 1부터 20까지의 숫자로 토큰화되었다는 것을 알 수 있습니다.
그런데 입력된 리뷰 데이터의 토큰 수가 각각 다르네요. 
딥러닝 모델에 입력하려면 학습 데이터의 길이가 동일해야 합니다.
따라서 토큰의 수를 똑같이 맞추어 주어야 합니다.
이처럼 길이를 똑같이 맞추어 주는 작업을 **패딩** 과정이라고 합니다.
패딩 작업을 위해 케라스는 **pad_sequences()** 함수를 제공합니다.
원하는 길이보다 짧은 부분은 숫자 0을 넣어서 채워 주고, 긴 데이터는 잘라서 같은 길이로 맞춥니다. 

![image](https://user-images.githubusercontent.com/52357235/178222222-d577c6a0-5a5e-48eb-a59a-2f3d81991cce.png)

![image](https://user-images.githubusercontent.com/52357235/178222234-11546f1d-86bf-4ea5-8fac-63bd64200b6e.png)

이제 단어 임베딩을 포함해 딥러닝 모델을 만들고 결과를 출력해 보겠습니다.
임베딩 함수에 필요한 세 가지 파라미터는 '입력, 출력, 단어 수'입니다.
총 몇 개의 단어 집합에서(입력), 몇 개의 임베딩 결과를 사용할 것인지(출력), 그리고 매번 입력될 단어 수는 몇 개로 할지(단어 수)를 정해야 하는 것입니다.
먼저 총 몇 개의 인덱스가 '입력'되어야 하는지 정합니다.
word_size라는 변수를 만든 후 길이를 세는 len() 함수를 이용해 word_index 값을 앞서 만든 변수에 대입합니다.
이때 전체 단어의 맨 앞에 0이 먼저 나와야 하므로 총 단어 수에 1을 더하는 것을 잊지 마시기 바랍니다.

![image](https://user-images.githubusercontent.com/52357235/178223140-0ef956df-35ac-495e-b0cf-5a751fec671a.png)

이제 몇 개의 임베딩 결과를 사용할 것인지, 즉 '출력'을 정할 차례입니다.
이번 예제에서는 word_size만큼 입력 값을 이용해 여덟 개의 임베딩 결과를 만들겠습니다.
여기서 8이라는 숫자는 임의로 정한 것입니다. 
데이터에 따라 적절한 값으로 바꿀 수 있습니다. 
이때 만들어진 여덟 개의 임베딩 결과는 우리 눈에 보이지 않습니다.
내부에서 게산해 딥러닝의 층으로 활용됩니다.
끝으로 매번 입력될 '단어 수'를 정합니다.
패딩 과정을 거쳐 네 개의 길이로 맞추어 주었으므로 네 개의 단어가 들어가게 설정하면 임베딩 과정은 다음 한 줄로 표현됩니다.

![image](https://user-images.githubusercontent.com/52357235/178224070-248cf3f2-df9f-4ca0-be61-07c8e0763adc.png)

이를 이용해 모델을 만들면 다음과 같습니다.

![image](https://user-images.githubusercontent.com/52357235/178224931-8063065a-7935-4be7-91d2-75c1ecb0137c.png)

최적화 함수로 adam()을 사용하고 오차 함수로는 binary_crossentropy()를 사용했습니다.
그리고 2번 반복하고 나서 정확도를 계산해 출력하게 했습니다.

## 실습1 영화 리뷰가 긍정적인지 부정적인지를 예측하기
[실습1 영화 리뷰가 긍정적인지 부정적인지를 예측하기](https://github.com/zzzangmans1/DeepLearning/blob/main/17/17.py)

![image](https://user-images.githubusercontent.com/52357235/178229268-be0810b9-a9aa-4855-a7ff-c5698ab8f391.png)
