# 18장 시퀀스 배열로 다루는 순환 신경망(RNN)

인공지능이 문장을 듣고 이해한다는 것은 많은 문장을 '이미 학습(train)해 놓았다'는 것입니다.
그런데 문장을 학습하는 것은 우리가 지금까지 공부한 내용과는 성질이 조금 다릅니다.
문장은 여러 개의 단어로 이루어져 있는데, 그 의미를 전달하려면 각 단어가 정해진 순서대로 입력되어야 하기 때문입니다.
즉, 여러 데이터가 순서와 관계없이 입력되던 것과는 다르게, 이번에는 과거에 입력된 데이터와 나중에 입력된 사이의 관계를 고려해야 하는 문제가 생기는 것입니다.
이를 해결하기 위해 **순환 신경망**(Recurrent Neural Network, RNN) 방법이 고안되었습니다.
순환 신경망은 여러 개의 데이터가 순서대로 입력되었을 때 앞서 입력받은 데이터를 잠시 기억해 놓는 방법입니다.
그리고 기억된 데이터가 얼마나 중요한지 판단하고 별도의 가중치를 주어 다음 데이터로 넘어갑니다.
모든 입력 값에 이 작업을 순서대로 실행하므로 다음 층으로 넘어가기 전에 같은 층을 맴도는 것처럼 보입니다.
이렇게 같은 층 안에서 맵도는 성질 때문에 순환 신경망(이하 RNN)이라고 합니다.

![image](https://user-images.githubusercontent.com/52357235/178678101-f7dc3a88-bebe-42cb-bf45-da3ecd898a47.png)

RNN이 처음 개발된 이후, RNN의 결과를 더욱 개선하기 위한 노력이 계속되어 왔습니다.
이 중에서 LSTM(Long Short Term Memory) 방법을 함께 사용하는 기법이 현재 가장 널리 사용되고 있습니다.
LSTM은 한 층 안에서 반복을 많이 해야 하는 RNN의 특성상 일반 신경망보다 기울기 소실문제가 더 많이 발생하고 이를 해결하기 어렵다는 단점을 보완한 방법입니다.
즉, 반복되기 직전에 다음 층으로 기억된 값을 넘길지 여부를 관리하는 단계를 하나 더 추가하는 것입니다.

RNN 방식의 장점은 입력 값과 출력 값을 어떻게 설정하느냐에 따라 여러 가지 상황에서 이를 적용할 수 있다는 것입니다.

- 다수 입력 단일 출력
: 문장을 읽고 뜻을 파악할 때 활용
- 단일 입력 다수 출력
: 사진의 캡션을 만들 때 활용
- 다수 입력 다수 출력
: 문장을 번역할 때 활용

케라스는 딥러닝 학습에 필요한 데이터를 쉽게 내려받을 수 있게 load_data() 함수를 제공합니다.
앞서 살펴본 MNIST 데이터셋 외에도 RNN 학습에 적절한 텍스트 대용량 데이터를 제공합니다.
케라스가 제공하는 '로이터 뉴스 카테고리 분류'와 IMDB 영화 리뷰'를 통해 지금부터 RNN을 학습해 보겠습니다.

## LSTM을 이용한 로이터 뉴스 카테고리 분류하기

입력된 문장 의미를 파악하는 것은 곧 모든 단어를 종합해 하나의 카테고리로 분류하는 작업이라고 할 수 있습니다.

|input|output|
|--|--|
|중부 지방은 대체로 맑겠으나, 남주 지방은 구름이 많겠습니다.|날씨|
|올 초부터 유동성의 힘으로 주가가 일정하게 상승했습니다.|주식|
|이번 선거에서는 누가 이길 것 같아?|정치|
|퍼셉트론의 한계를 극복한 신경망이 다시 뜨고 있대.|딥러닝|

이번에 실습할 내용은 이처럼 긴 텍스트를 읽고 이 데이터가 어떤 의미를 지니는지 카테고리로 분류하는 연습입니다.
실습을 위해 로이터 뉴스 데이터를 사용하겠습니다.
로이터 뉴스 데이터는 총 1만 1,228개의 뉴스 기사가 46개의 카테고리로 나누어진 대용량 텍스트 데이터입니다.

![image](https://user-images.githubusercontent.com/52357235/178681316-768c7fdd-e230-45c3-950b-b6e86f7d1676.png)

다음과 같이 불러온 데이터를 학습셋과 테스트셋으로 나누겠습니다.

![image](https://user-images.githubusercontent.com/52357235/178681739-9a40b071-78e5-46ee-bb6b-6badb6367ea8.png)

reuters.load_data() 함수를 이용해 기사를 불러왔습니다.
test_split 옵션을 통해 20%만 테스트셋으로 사용하겠다고 지정했습니다.
여기서 num_words 옵션은 무엇을 의미하는지 알아보고자 먼저 불러온 데이터에 대해 몇 가지를 출력해 보겠습니다.

![image](https://user-images.githubusercontent.com/52357235/178682801-cf97e914-8b4e-4690-8108-1e8388f37b31.png)

![image](https://user-images.githubusercontent.com/52357235/178682840-e24236fe-8552-4c0e-95a8-cfe90f6092e9.png)

먼저 np.max() 함수로 y_train의 종류를 구하니 46개의 카테고리로 구분되어 있음을 알 수 있었습니다(0부터 세기 때문에 1을 더해서 출력합니다).
이 중 8,982개는 학습용으로, 2,246개는 테스트용으로 준비되어 있습니다.
그런데 print(X_train[0])으로 기사를 출력해 보니 단어가 나오지 않고[1, 2, 2, 8, 43...]같은 숫자가 나옵니다.
이 처럼 딥러닝은 단어를 그대로 사용하지 않고 숫자로 변환한 후 학습할 수 있습니다.
여기서는 데이터 안에서 해당 단어가 몇 번이나 나타나는지 세어 빈도에 따라 번호를 붙였습니다.
예를 들어 3이라고 하면 세 번째로 빈도가 높은 단어라는 의미입니다.
이러한 작업을 위해 tokenizer() 같은 함수를 사용하는데, 케라스는 이 작업을 이미 마친 데이터를 불러올 수 있습니다.
빈도가 높은 단어만 불러와 사용하는 것이 **num_words=1000**의 의미입니다.
빈도가 1~1,000에 해당하는 단어만 선택해서 불러오는 것입니다.
또 하나 주의해야 할 점은 각 기사의 단어 수가 제각각 다르므로 이를 동일하게 맞추어야 한다는 것입니다.
이때는 다음과 같이 데이터 전처리 함수 sequence()를 이용합니다.

![image](https://user-images.githubusercontent.com/52357235/178684848-9e427d3a-90fd-45b6-b67c-6065cdb46fcd.png)

여기서 maxlen=100은 단어 수를 100개로 맞추라는 의미입니다.
만일 입력된 기사의 단어 수가 100보다 크면 100개째 단어만 선택하고 나머지는 버립니다.
100에서 모자랄 때는 모자라는 부분을 모두 0으로 채웁니다.
이제 y 데이터에 원-핫 인코딩 처리를 하여 데이터 전처리 과정을 마칩니다.

![image](https://user-images.githubusercontent.com/52357235/178685581-6d2683f6-9f7e-4907-89f8-fc053490c74d.png)

데이터 전처리 과정이 끝났으므로 딥러닝의 구조를 만들 차례입니다.

![image](https://user-images.githubusercontent.com/52357235/178686158-8a50d3a9-bfde-4512-a8a4-72f16a64233f.png)

Embedding 층과 LSTM 층을 새로 추가했습니다.
Embedding 층은 데이터 전처리 과정을 통해 입력된 값을 받아 다음 층이 알 수 있는 형태로 변환하는 역할을 합니다.
Embedding('불러온 단어의 총 수', '기사당 단어 수') 형식으로 사용하며, 모델 설정 부분의 맨 처음에 있어야 합니다.
LSTM은 앞서 설명했듯이 RNN에서 기억 값에 대한 가중치를 제어하며, LSTM(기사당 단어수, 기타 옵션) 형식으로 적용됩니다.
LSTM의 활성화 함수로는 tanh를 주로 사용하므로 actiavation='tanh'로 지정했습니다.

![image](https://user-images.githubusercontent.com/52357235/178689478-6713f858-9ef9-41b2-a0ff-78e75ece75d4.png)

[실습! LSTM을 이용해 로이터 뉴스 카테고리 분석하기](https://github.com/zzzangmans1/DeepLearning/blob/main/18/18_1.py)

![image](https://user-images.githubusercontent.com/52357235/178694382-e38258cf-72da-44a4-9951-ffd679817de6.png)

## LSTM과 CNN의 조합을 이용한 영화 리뷰 분류하기

이번에 사용할 인터넷 영화 데이터베이스(Internet Movie DataBase, IMDB)는 영화와 관련된 정보와 출연진 정보, 개봉 정보, 영화 후기, 평점까지 매우 폭넓은 데이터가 저장된 자료입니다.
영화에 관해 남긴 2만 5,000여 개의 영화 리뷰가 담겨 있으며, 해당 영화를 긍정적으로 평가했는지 혹은 부정적으로 평가했는지도 담겨 있습니다.
앞서 다루었던 로이터뉴스 데이터와 마찬가지로 각 단어에 대한 전처리를 마친 상태입니다.
데이터셋에서 나타나는 빈도에 따라 번호가 정해지므로 빈도가 높은 데이터를 불러와 학습시킬 수 있습니다.
데이터 전처리 과정은 로이터 뉴스 데이터와 거의 같습니다.
다만 클래스가 긍정 또는 부정 두 가지뿐이라 원-핫 인코딩 과정이 없습니다.

![image](https://user-images.githubusercontent.com/52357235/178694845-f7f86220-49b0-41f1-9c9b-3991436458b6.png)

이제 모델을 다음과 같이 설정합니다.
model.summary() 함수를 이용해 현재 설정된모델의 구조를 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/52357235/178696574-0222c693-a0e3-4511-89d6-a567acf9e5e8.png)

![image](https://user-images.githubusercontent.com/52357235/178696604-5e06e8ce-e669-429d-b4f7-6dbb7afc405a.png)

출력 결과에서 우리가 아직 보지 못한 부분은 Conv1D와 MaxPooling1D입니다.
앞서 Conv2D와 MaxPooling2D는 앞에서 다루었습니다.
하지만 2차원 배열을 가진 이미지와 다르게 지금 다루고 있는 데이터는 배열 형태로 이루어진 1차원이라는 차이가 있습니다.
Conv1D는 Conv2D의 개념을 1차원으로 옮긴 것입니다.
컨볼루션 층이 1차원이고 이동하는 배열도 1차원입니다.

![image](https://user-images.githubusercontent.com/52357235/178698313-5a63c213-db82-4780-9062-0eb4bbcf3a70.png)

[실습1 LSTM과 CNN을 조합해 영화 리뷰 분류하기](https://github.com/zzzangmans1/DeepLearning/blob/main/18/18_2.py)

![image](https://user-images.githubusercontent.com/52357235/178701095-b752ca4a-cf41-43e5-9bfe-14ae22c15621.png)
